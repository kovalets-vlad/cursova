import os
import pandas as pd
import numpy as np
import joblib
import shap
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error
from keras.models import Sequential
from keras.layers import GRU, Dense, Dropout, Input, Conv1D, LSTM, BatchNormalization, GlobalAveragePooling1D, Activation
from joblib import Parallel, delayed
from keras.optimizers import Adam

# ==========================================
# 1. –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø
# ==========================================
dynamic_cols = ['steps', 'very_active_minutes', 'minutesAsleep', 'sleep_efficiency', 
                'nremhr', 'stress_score', 'nightly_temperature', 'resting_hr',
                'chronic_steps', 'acute_steps', 'acwr']
static_cols = ['age', 'bmi']
weekend_col = ['is_weekend']
target_col = 'hr_delta' 
DAYS_WINDOW = 3 

MODEL_TYPES = ['GRU', 'LSTM', 'CNN']
BASE_OUTPUT_DIR = 'cursova/models_ensemble'

# ==========================================
# 2. –§–£–ù–ö–¶–Ü–á –ú–û–î–ï–õ–ï–ô –¢–ê –û–ë–†–û–ë–ö–ò
# ==========================================

def build_model(input_shape, model_type='GRU'):
    model = Sequential()
    model.add(Input(shape=input_shape))
    if model_type == 'GRU':
        model.add(GRU(64, return_sequences=True))
        model.add(BatchNormalization())
        model.add(Dropout(0.3))
        model.add(GRU(64))
    elif model_type == 'LSTM':
        model.add(LSTM(128, return_sequences=True)) 
        model.add(BatchNormalization())
        model.add(Dropout(0.3))
        model.add(LSTM(64))
        model.add(BatchNormalization())

        model.add(Dense(64, activation='relu'))
        model.add(Dense(32, activation='relu'))
    elif model_type == 'CNN':
        model.add(Conv1D(filters=64, kernel_size=2, padding='same'))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(Conv1D(filters=128, kernel_size=2, padding='same'))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(GlobalAveragePooling1D())

    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1)) 
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
    return model

def process_user_with_delta(df, user_id, dynamic_cols, static_cols, scaler_X, scaler_Y):
    user_df = df[df['id'] == user_id].copy()
    user_df['age'] = pd.to_numeric(user_df['age'], errors='coerce').fillna(30)
    user_df['bmi'] = pd.to_numeric(user_df['bmi'], errors='coerce').fillna(25)
    
    # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –º–µ—Ç—Ä–∏–∫
    user_df['chronic_steps'] = user_df['steps'].rolling(window=28, min_periods=1).mean()
    user_df['acute_steps'] = user_df['steps'].rolling(window=7, min_periods=1).mean()
    user_df['acwr'] = user_df['acute_steps'] / (user_df['chronic_steps'] + 1)
    user_df['hr_delta'] = user_df['resting_hr'].diff().fillna(0)
    
    user_df = user_df.ffill().bfill()
    user_df = user_df.iloc[1:].reset_index(drop=True)

    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è —á–µ—Ä–µ–∑ –ì–õ–û–ë–ê–õ–¨–ù–Ü —Å–∫–∞–ª–µ—Ä–∏
    dyn_scaled = scaler_X.transform(user_df[dynamic_cols].values)
    stat_data = user_df[static_cols].values.astype(float)
    stat_data[:, 0] /= 100.0
    stat_data[:, 1] /= 50.0
    X_final = np.hstack((dyn_scaled, stat_data, user_df[['is_weekend']].values))
    
    y_scaled = scaler_Y.transform(user_df[[target_col]].values)
    # –ü–û–í–ï–†–¢–ê–Ñ–ú–û –†–Ü–í–ù–û 3 –ó–ù–ê–ß–ï–ù–ù–Ø
    return X_final, y_scaled, user_df['resting_hr'].values

def prepare_data_from_users(user_list, df, dynamic_cols, static_cols, scaler_X, scaler_Y, days_to_skip_at_end=0):
    X_list, y_list, base_hr_list = [], [], []
    for u in user_list:
        X_u, y_u_sc, raw_hr = process_user_with_delta(df, u, dynamic_cols, static_cols, scaler_X, scaler_Y)
        
        limit = len(X_u) - days_to_skip_at_end
        
        if limit > DAYS_WINDOW:
            for i in range(limit - DAYS_WINDOW):
                X_list.append(X_u[i : i + DAYS_WINDOW])
                y_list.append(y_u_sc[i + DAYS_WINDOW])
                base_hr_list.append(raw_hr[i + DAYS_WINDOW - 1])
                
    return np.array(X_list), np.array(y_list), np.array(base_hr_list)

# ==========================================
# 3. –û–°–ù–û–í–ù–ò–ô –¶–ò–ö–õ –ù–ê–í–ß–ê–ù–ù–Ø
# ==========================================
if __name__ == "__main__":
    df = pd.read_csv('cursova/daily_fitbit_sema_df_processed.csv')
    df['date'] = pd.to_datetime(df['date'])
    df['is_weekend'] = (df['date'].dt.dayofweek >= 5).astype(int)

    # –ü–æ–ø–µ—Ä–µ–¥–Ω—è –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è —Å–∫–∞–ª–µ—Ä—ñ–≤
    df['chronic_steps'] = df.groupby('id')['steps'].transform(lambda x: x.rolling(window=28, min_periods=1).mean())
    df['acute_steps'] = df.groupby('id')['steps'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())
    df['acwr'] = df['acute_steps'] / (df['chronic_steps'] + 1)
    df[['chronic_steps', 'acute_steps', 'acwr']] = df[['chronic_steps', 'acute_steps', 'acwr']].ffill().bfill()

    all_users = df['id'].unique().tolist()
    user_folds = np.array_split(all_users, 5) 

    # --- –ö–†–û–ö 1: –ì–õ–û–ë–ê–õ–¨–ù–Ü –°–ö–ï–õ–ï–†–ò ---
    print("üìè –ù–∞–≤—á–∞–Ω–Ω—è –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö —Å–∫–∞–ª–µ—Ä—ñ–≤...")
    global_scaler_X = StandardScaler().fit(df[dynamic_cols].values)
    all_deltas = df.groupby('id')['resting_hr'].diff().fillna(0).values.reshape(-1, 1)
    global_scaler_Y = StandardScaler().fit(all_deltas)

    # --- –ö–†–û–ö 2: –ù–ê–í–ß–ê–ù–ù–Ø –¢–ê –ê–ù–ê–õ–Ü–ó ---
    for m_type in MODEL_TYPES:
        # 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–ª–æ–≤–Ω–∏–∫–∞ –º–µ—Ç—Ä–∏–∫ 
        history_metrics = {'folds': [], 'train_mae': [], 'test_mae': [], 'test_r2': []}
        model_dir = os.path.join(BASE_OUTPUT_DIR, m_type.lower())
        os.makedirs(model_dir, exist_ok=True)
        
        test_group = user_folds[4]
        X_test, y_test, base_hr_test = prepare_data_from_users(test_group, df, dynamic_cols, static_cols, global_scaler_X, global_scaler_Y)

        for n_folds in range(1, 5):
            print(f"üìä –ù–∞–≤—á–∞–Ω–Ω—è –Ω–∞ {n_folds} —Ñ–æ–ª–¥–∞—Ö...")
            train_group = np.concatenate([user_folds[i] for i in range(n_folds)])
            X_train, y_train, base_hr_train = prepare_data_from_users(train_group, df, dynamic_cols, static_cols, global_scaler_X, global_scaler_Y)
            
            model = build_model((DAYS_WINDOW, X_train.shape[2]), m_type)
            model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)
            
            # --- –†–û–ó–†–ê–•–£–ù–û–ö TRAIN MAE (–≤ BPM) ---
            train_preds_z = model.predict(X_train, verbose=0)
            train_pred_bpm = base_hr_train + global_scaler_Y.inverse_transform(train_preds_z).flatten()
            train_real_bpm = base_hr_train + global_scaler_Y.inverse_transform(y_train.reshape(-1, 1)).flatten()
            train_mae_val = mean_absolute_error(train_real_bpm, train_pred_bpm)
            
            # --- –†–û–ó–†–ê–•–£–ù–û–ö TEST MAE (–≤ BPM) ---
            test_preds_z = model.predict(X_test, verbose=0)
            test_pred_bpm = base_hr_test + global_scaler_Y.inverse_transform(test_preds_z).flatten()
            test_real_bpm = base_hr_test + global_scaler_Y.inverse_transform(y_test.reshape(-1, 1)).flatten()
            test_mae_val = mean_absolute_error(test_real_bpm, test_pred_bpm)
            test_r2_val = r2_score(test_real_bpm, test_pred_bpm)

            # –î–û–î–ê–í–ê–ù–ù–Ø –í –°–ü–ò–°–ö–ò (–ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ, —â–æ–± —Ü—ñ —Ä—è–¥–∫–∏ –≤–∏–∫–æ–Ω—É–≤–∞–ª–∏—Å—å –Ω–∞ –∫–æ–∂–Ω—ñ–π —ñ—Ç–µ—Ä–∞—Ü—ñ—ó)
            history_metrics['folds'].append(n_folds)
            history_metrics['train_mae'].append(train_mae_val)
            history_metrics['test_mae'].append(test_mae_val)
            history_metrics['test_r2'].append(test_r2_val)

            if n_folds == 4:
                X_final_test = X_test  # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–ª—è SHAP –∞–Ω–∞–ª—ñ–∑—É –ø—ñ–∑–Ω—ñ—à–µ
                final_model = model  # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó –º–æ–¥–µ–ª—ñ
                final_model.save(os.path.join(model_dir, f'final_{m_type.lower()}_model.keras'))

        # --- –ü–û–ë–£–î–û–í–ê –ì–†–ê–§–Ü–ö–ê ---
        fig, ax1 = plt.subplots(figsize=(10, 6))
        ax1.set_xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ–æ–ª–¥—ñ–≤ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è')
        ax1.set_ylabel('MAE (BPM)', color='black')
        
        # –û—Å–Ω–æ–≤–Ω—ñ –ª—ñ–Ω—ñ—ó
        ax1.plot(history_metrics['folds'], history_metrics['test_mae'], 
                marker='o', color='tab:red', linewidth=2, label='Test MAE (BPM)')
        ax1.plot(history_metrics['folds'], history_metrics['train_mae'], 
                marker='s', color='tab:green', linestyle='--', linewidth=2, label='Train MAE (BPM)')
        
        ax1.legend(loc='upper right')
        ax1.grid(True, alpha=0.3)
        plt.title(f"Learning Curve: {m_type}")

        # 1. –°–ü–ï–†–®–£ –ó–ë–ï–†–Ü–ì–ê–Ñ–ú–û 
        save_path = os.path.join(model_dir, f'{m_type.lower()}_learning_curve.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"üíæ –ì—Ä–∞—Ñ—ñ–∫ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path}")

        # 2. –ü–û–¢–Ü–ú –ü–û–ö–ê–ó–£–Ñ–ú–û
        plt.show()

        # 3. –ó–ê–ö–†–ò–í–ê–Ñ–ú–û 
        plt.close(fig)

        # # --- –ö–†–û–ö 3: SHAP –ê–ù–ê–õ–Ü–ó ---
        print(f"üîç –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ SHAP...")
        bg_idx = np.random.choice(X_final_test.shape[0], 100, replace=False)
        # –§—É–Ω–∫—Ü—ñ—è-–æ–±–≥–æ—Ä—Ç–∫–∞ –¥–ª—è SHAP, —è–∫–∞ —Ä–æ–∑–≥–æ—Ä—Ç–∞—î 2D –¥–∞–Ω—ñ –Ω–∞–∑–∞–¥ —É 3D
        def predict_for_shap(x_flat):
            x_3d = x_flat.reshape(-1, DAYS_WINDOW, X_final_test.shape[2])
            return final_model.predict(x_3d, verbose=0)

        explainer = shap.KernelExplainer(predict_for_shap, shap.kmeans(X_final_test[bg_idx].reshape(100, -1), 10))
        test_idx = np.random.choice(X_final_test.shape[0], 30, replace=False)
        shap_values = explainer.shap_values(X_final_test[test_idx].reshape(30, -1))
        
        # –û–±—Ä–æ–±–∫–∞ –≤–∏—Ö–æ–¥—É SHAP (–¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó —Ü–µ –∑–∞–∑–≤–∏—á–∞–π —Å–ø–∏—Å–æ–∫ –∑ –æ–¥–Ω–æ–≥–æ –º–∞—Å–∏–≤—É –∞–±–æ –ø—Ä–æ—Å—Ç–æ –º–∞—Å–∏–≤)
        if isinstance(shap_values, list): sv = shap_values[0]
        else: sv = shap_values

        plt.figure(figsize=(10, 6))
        # –£—Å–µ—Ä–µ–¥–Ω—é—î–º–æ SHAP-–∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ —á–∞—Å–æ–≤–∏–º –≤—ñ–∫–Ω–æ–º (3 –¥–Ω—ñ)
        shap.summary_plot(np.mean(sv.reshape(-1, DAYS_WINDOW, X_final_test.shape[2]), axis=1), 
                          np.mean(X_final_test[test_idx], axis=1), 
                          feature_names=dynamic_cols+static_cols+weekend_col, show=False)
        plt.savefig(os.path.join(model_dir, 'shap_summary.png'))
        plt.close()

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
        final_model.save(os.path.join(model_dir, f'{m_type.lower()}_model.keras'))
        joblib.dump(global_scaler_X, os.path.join(model_dir, 'scaler_X.pkl'))
        joblib.dump(global_scaler_Y, os.path.join(model_dir, 'scaler_Y.pkl'))
        joblib.dump(dynamic_cols+static_cols+weekend_col, os.path.join(model_dir, 'features.pkl'))
        print(f"‚úÖ {m_type} –≥–æ—Ç–æ–≤o!")