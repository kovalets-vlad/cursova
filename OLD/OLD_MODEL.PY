import pandas as pd
import numpy as np
import shap
import joblib
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from keras.models import Sequential
from keras.layers import GRU, Dense, Dropout, Input, LSTM, Conv1D, Flatten, MaxPooling1D
from joblib import Parallel, delayed

dynamic_cols = [
    'steps', 'very_active_minutes', 'minutesAsleep', 'sleep_efficiency', 
    'nremhr', 'stress_score', 'nightly_temperature', 'resting_hr', 'is_weekend'
]
static_cols = ['age', 'bmi'] 
target_col = 'resting_hr'

# ==========================================
# 2. –î–û–ü–û–ú–Ü–ñ–ù–Ü –§–£–ù–ö–¶–Ü–á
# ==========================================
def create_dataset(dataset, target_index, time_steps=3):
    X, Y = [], []
    for i in range(len(dataset) - time_steps - 1):
        X.append(dataset[i:(i + time_steps), :])
        Y.append(dataset[i + time_steps, target_index])
    return np.array(X), np.array(Y)

def build_model(input_shape, model_type='GRU'):
    model = Sequential()
    model.add(Input(shape=input_shape))
    
    if model_type == 'GRU':
        model.add(GRU(64, return_sequences=True))
        model.add(Dropout(0.3))
        model.add(GRU(64))
        
    elif model_type == 'LSTM':
        model.add(LSTM(64, return_sequences=True))
        model.add(Dropout(0.3))
        model.add(LSTM(64))
        
    elif model_type == 'CNN':
        # CNN –ª—é–±–∏—Ç—å –±–∞—á–∏—Ç–∏ –ø–∞—Ç–µ—Ä–Ω–∏
        model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))
        model.add(MaxPooling1D(pool_size=1)) # –ú–æ–∂–Ω–∞ –ø—Ä–∏–±—Ä–∞—Ç–∏, —è–∫—â–æ –≤—ñ–∫–Ω–æ –º–∞–ª–µ
        model.add(Flatten())
        model.add(Dense(50, activation='relu'))

    model.add(Dropout(0.3))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

def process_user_data(df, user_id, dynamic_cols, static_cols, scaler=None):
    user_df = df[df['id'] == user_id].copy().ffill().bfill()
    
    # 1. –î–∏–Ω–∞–º—ñ–∫–∞
    dyn_data = user_df[dynamic_cols].values
    if scaler is None:
        scaler = StandardScaler()
        dyn_scaled = scaler.fit_transform(dyn_data)
    else:
        dyn_scaled = scaler.transform(dyn_data)
        
    # 2. –°—Ç–∞—Ç–∏–∫–∞
    try:
        stat_data = user_df[static_cols].values
        stat_data[:, 0] = stat_data[:, 0] / 100.0 # Age
        stat_data[:, 1] = stat_data[:, 1] / 50.0  # BMI
    except KeyError:
        stat_data = np.zeros((len(user_df), len(static_cols)))

    final_data = np.hstack((dyn_scaled, stat_data))
    return final_data, scaler

# ==========================================
# 3. –§–£–ù–ö–¶–Ü–Ø –î–õ–Ø –ü–ê–†–ê–õ–ï–õ–¨–ù–û–ì–û –ó–ê–ü–£–°–ö–£
# ==========================================
def evaluate_single_fold(test_user, all_users, df, dynamic_cols, static_cols, target_idx):
    """
    –¶—è —Ñ—É–Ω–∫—Ü—ñ—è –±—É–¥–µ –∑–∞–ø—É—Å–∫–∞—Ç–∏—Å—è –Ω–∞ –æ–∫—Ä–µ–º–æ–º—É —è–¥—Ä—ñ –ø—Ä–æ—Ü–µ—Å–æ—Ä–∞.
    –í–æ–Ω–∞ —Ç—Ä–µ–Ω—É—î –º–æ–¥–µ–ª—å —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –ª–∏—à–µ –º–µ—Ç—Ä–∏–∫–∏ (MAE, R2).
    """
    # –§–æ—Ä–º—É—î–º–æ TRAIN
    train_users = [u for u in all_users if u != test_user]
    X_train_list, y_train_list = [], []
    
    for train_user in train_users:
        processed_data, _ = process_user_data(df, train_user, dynamic_cols, static_cols, scaler=None)
        X_u, y_u = create_dataset(processed_data, target_index=target_idx, time_steps=3)
        X_train_list.append(X_u)
        y_train_list.append(y_u)
        
    X_train = np.concatenate(X_train_list, axis=0)
    y_train = np.concatenate(y_train_list, axis=0)
    
    # –§–æ—Ä–º—É—î–º–æ TEST
    test_data_processed, test_scaler = process_user_data(df, test_user, dynamic_cols, static_cols, scaler=None)
    X_test, y_test = create_dataset(test_data_processed, target_index=target_idx, time_steps=3)
    
    # –ù–∞–≤—á–∞–Ω–Ω—è (–ª–æ–∫–∞–ª—å–Ω–µ –¥–ª—è –ø—Ä–æ—Ü–µ—Å—É)
    model = build_model((X_train.shape[1], X_train.shape[2]))
    # verbose=0 –æ–±–æ–≤'—è–∑–∫–æ–≤–æ, —â–æ–± –Ω–µ –∑–º—ñ—à—É–≤–∞–ª–∏—Å—è –ø—Ä–∏–Ω—Ç–∏ –∑ —Ä—ñ–∑–Ω–∏—Ö –ø–æ—Ç–æ–∫—ñ–≤
    model.fit(X_train, y_train, epochs=25, batch_size=16, verbose=0) 
    
    # –û—Ü—ñ–Ω–∫–∞
    preds_scaled = model.predict(X_test, verbose=0)
    
    # –Ü–Ω–≤–µ—Ä—Å—ñ—è
    dummy_matrix = np.zeros((len(preds_scaled), len(dynamic_cols)))
    dummy_matrix[:, target_idx] = preds_scaled.flatten() 
    preds_real = test_scaler.inverse_transform(dummy_matrix)[:, target_idx]
    
    dummy_matrix[:, target_idx] = y_test.flatten()
    y_real = test_scaler.inverse_transform(dummy_matrix)[:, target_idx]
    
    mae = mean_absolute_error(y_real, preds_real)
    mse = mean_squared_error(y_real, preds_real)
    r2 = r2_score(y_real, preds_real)
    
    
    # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ ID —é–∑–µ—Ä–∞, —â–æ–± –∑–Ω–∞—Ç–∏, —á–∏–π —Ü–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    return test_user, mae, mse, r2

# ==========================================
# 4. –ì–û–õ–û–í–ù–ò–ô –ë–õ–û–ö –í–ò–ö–û–ù–ê–ù–ù–Ø
# ==========================================

if __name__ == "__main__":
    df = pd.read_csv('cursova/daily_fitbit_sema_df_processed.csv') 
    
    # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø –¢–ò–ü–Ü–í (–≤–∞–∂–ª–∏–≤–µ –¥–ª—è –≤–∞—à–æ—ó –ø–æ–º–∏–ª–∫–∏ –∑ –¥—ñ–ª–µ–Ω–Ω—è–º)
    df['age'] = pd.to_numeric(df['age'], errors='coerce').fillna(30) # –ó–∞–ø–æ–≤–Ω—é—î–º–æ —Å–µ—Ä–µ–¥–Ω—ñ–º, —è–∫—â–æ –ø—É—Å—Ç–æ
    df['bmi'] = pd.to_numeric(df['bmi'], errors='coerce').fillna(25)
    df['date'] = pd.to_datetime(df['date'])
    df['is_weekend'] = (df['date'].dt.dayofweek >= 5).astype(int)

    target_idx = dynamic_cols.index(target_col) 
    
    # –ë–µ—Ä–µ–º–æ –í–°–Ü–• —é–∑–µ—Ä—ñ–≤
    all_users = df['id'].unique().tolist()
    np.random.shuffle(all_users) # –ü–µ—Ä–µ–º—ñ—à—É—î–º–æ, —â–æ–± –±—É–ª–æ —á–µ—Å–Ω–æ
    
    # –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ 5 —á–∞—Å—Ç–∏–Ω (Folds)
    n_folds = 5
    # np.array_split —Ä–æ–∑—Ä—ñ–∂–µ —Å–ø–∏—Å–æ–∫ —é–∑–µ—Ä—ñ–≤ –Ω–∞ 5 —à–º–∞—Ç–∫—ñ–≤
    user_folds = np.array_split(all_users, n_folds)
    
    print(f"–ó–∞–ø—É—Å–∫–∞—î–º–æ 5-Fold CV –Ω–∞ {len(all_users)} –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞—Ö...")
    print(f"–ö–æ–∂–µ–Ω —Ñ–æ–ª–¥ –º—ñ—Å—Ç–∏—Ç—å ~{len(user_folds[0])} —é–∑–µ—Ä—ñ–≤.")

    # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–æ–ª–¥–∞ (—Ç–µ–ø–µ—Ä –≤–æ–Ω–∞ –æ–±—Ä–æ–±–ª—è—î –ì–†–£–ü–£ —é–∑–µ—Ä—ñ–≤)
    def evaluate_fold_group(fold_idx, folds, df, dynamic_cols, static_cols, target_idx):
        # Test users - —Ü–µ –ø–æ—Ç–æ—á–Ω–∞ –≥—Ä—É–ø–∞
        test_users_group = folds[fold_idx]
        
        # Train users - —Ü–µ –≤—Å—ñ —ñ–Ω—à—ñ –≥—Ä—É–ø–∏ —Ä–∞–∑–æ–º
        train_users_group = np.concatenate([folds[i] for i in range(n_folds) if i != fold_idx])
        
        # 1. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ TRAIN (–≤–µ–ª–∏–∫–∞ –∫—É–ø–∞ –¥–∞–Ω–∏—Ö)
        X_train_list, y_train_list = [], []
        for u in train_users_group:
            # scaler=None -> –≤—á–∏–º–æ –Ω–æ–≤–∏–π —Å–∫–µ–π–ª–µ—Ä
            processed, _ = process_user_data(df, u, dynamic_cols, static_cols, scaler=None)
            X_u, y_u = create_dataset(processed, target_index=target_idx, time_steps=3)
            X_train_list.append(X_u)
            y_train_list.append(y_u)
        
        X_train = np.concatenate(X_train_list, axis=0)
        y_train = np.concatenate(y_train_list, axis=0)
        
        # 2. –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        model = build_model((X_train.shape[1], X_train.shape[2]), model_type='GRU')
        model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)
        
        # 3. –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è (–Ω–∞ –∫–æ–∂–Ω–æ–º—É —é–∑–µ—Ä—ñ –∑ —Ç–µ—Å—Ç–æ–≤–æ—ó –≥—Ä—É–ø–∏ –æ–∫—Ä–µ–º–æ)
        mae_scores = []
        mse_scores = []
        r2_scores = []
        
        for test_user in test_users_group:
            # –í–∞–∂–ª–∏–≤–æ: –¥–ª—è —Ç–µ—Å—Ç—É –º–∏ —Å—Ç–≤–æ—Ä—é—î–º–æ scaler –Ω–∞ –æ—Å–Ω–æ–≤—ñ –°–ê–ú–ï –¶–¨–û–ì–û —é–∑–µ—Ä–∞
            test_data, test_scaler = process_user_data(df, test_user, dynamic_cols, static_cols, scaler=None)
            X_test, y_test = create_dataset(test_data, target_index=target_idx, time_steps=3)
            
            if len(X_test) == 0: continue # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ, —è–∫—â–æ —É —é–∑–µ—Ä–∞ –º–∞–ª–æ –¥–∞–Ω–∏—Ö
            
            preds_scaled = model.predict(X_test, verbose=0)
            
            # –Ü–Ω–≤–µ—Ä—Å—ñ—è
            dummy = np.zeros((len(preds_scaled), len(dynamic_cols)))
            dummy[:, target_idx] = preds_scaled.flatten()
            preds_real = test_scaler.inverse_transform(dummy)[:, target_idx]
            
            dummy[:, target_idx] = y_test.flatten()
            y_real = test_scaler.inverse_transform(dummy)[:, target_idx]
            
            mae_scores.append(mean_absolute_error(y_real, preds_real))
            mse_scores.append(mean_squared_error(y_real, preds_real))
            r2_scores.append(r2_score(y_real, preds_real))
            
        return np.mean(mae_scores), np.mean(mse_scores), np.mean(r2_scores)

    # –ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ (–≤—Å—å–æ–≥–æ 5 –∑–∞–¥–∞—á, —Ü–µ –±—É–¥–µ —à–≤–∏–¥–∫–æ)
    # n_jobs=5 —ñ–¥–µ–∞–ª—å–Ω–æ –ª—è–∂–µ –Ω–∞ –≤–∞—à—ñ 6 —è–¥–µ—Ä
    results = Parallel(n_jobs=5)(
        delayed(evaluate_fold_group)(i, user_folds, df, dynamic_cols, static_cols, target_idx) 
        for i in range(n_folds)
    )
    
    # –í–∏–≤—ñ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    final_mae = [res[0] for res in results]
    final_mse = [res[1] for res in results]
    final_r2 = [res[2] for res in results]
    
    print("\n" + "="*30)
    print(f"–°–ï–†–ï–î–ù–Ü–ô –†–ï–ó–£–õ–¨–¢–ê–¢ (5-Fold CV):")
    print(f"MAE: {np.mean(final_mae):.2f}")
    print(f"MSE: {np.mean(final_mse):.2f}")
    print(f"RMSE: {np.sqrt(np.mean(final_mse)):.2f}")
    print(f"R2: {np.mean(final_r2):.4f}")

# ==========================================
    # 5. –ü–Ü–î–ì–û–¢–û–í–ö–ê –î–õ–Ø SHAP –¢–ê –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø
    # ==========================================
    print("\nüîÑ –ü–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó –º–æ–¥–µ–ª—ñ –¥–ª—è SHAP —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è...")
    
    # 1. –í–∏–±–∏—Ä–∞—î–º–æ –¥–∞–Ω—ñ (Strategy: Last Fold as Test)
    # user_folds[-1] - —Ü–µ –º–∞—Å–∏–≤ ID –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –∑ –æ—Å—Ç–∞–Ω–Ω—å–æ—ó –≥—Ä—É–ø–∏
    test_users_group = user_folds[-1]
    
    # –í—Å—ñ —ñ–Ω—à—ñ –≥—Ä—É–ø–∏ –æ–±'—î–¥–Ω—É—î–º–æ –≤ –æ–¥–∏–Ω –≤–µ–ª–∏–∫–∏–π –º–∞—Å–∏–≤ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
    # np.concatenate —Å–∫–ª–µ—ó—Ç—å 4 –º–∞—Å–∏–≤–∏ –≤ –æ–¥–∏–Ω –¥–æ–≤–≥–∏–π —Å–ø–∏—Å–æ–∫
    train_users_group = np.concatenate(user_folds[:-1])
    
    print(f"–§—ñ–Ω–∞–ª—å–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {len(train_users_group)} —é–∑–µ—Ä—ñ–≤ –≤ Train, {len(test_users_group)} –≤ Test")

    # 2. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ TRAIN (–ø—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –∫–æ–∂–Ω–æ–º—É —é–∑–µ—Ä—É –∑ –≤–µ–ª–∏–∫–æ—ó –≥—Ä—É–ø–∏)
    X_train_list, y_train_list = [], []
    for train_user in train_users_group:
        # –û–±—Ä–æ–±–ª—è—î–º–æ –∫–æ–∂–Ω–æ–≥–æ —é–∑–µ—Ä–∞ –æ–∫—Ä–µ–º–æ
        processed_data, _ = process_user_data(df, train_user, dynamic_cols, static_cols, scaler=None)
        X_u, y_u = create_dataset(processed_data, target_index=target_idx, time_steps=3)
        X_train_list.append(X_u)
        y_train_list.append(y_u)
    
    X_train = np.concatenate(X_train_list, axis=0)
    y_train = np.concatenate(y_train_list, axis=0)
    
    # 3. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ TEST (–¥–ª—è SHAP —ñ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å–∫–µ–π–ª–µ—Ä–∞)
    # –ù–∞–º –ø–æ—Ç—Ä—ñ–±–µ–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π scaler –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è, —Ç–æ–º—É –±–µ—Ä–µ–º–æ –ü–ï–†–®–û–ì–û —é–∑–µ—Ä–∞ –∑ —Ç–µ—Å—Ç–æ–≤–æ—ó –≥—Ä—É–ø–∏
    target_test_user = test_users_group[0]
    
    test_data_processed, test_scaler = process_user_data(df, target_test_user, dynamic_cols, static_cols, scaler=None)
    X_test, y_test = create_dataset(test_data_processed, target_index=target_idx, time_steps=3)
    
    # 4. –§—ñ–Ω–∞–ª—å–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è
    # –¢—É—Ç –º–∏ –≤–∂–µ –º–∞—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ
    model = build_model((X_train.shape[1], X_train.shape[2]))
    model.fit(X_train, y_train, epochs=25, batch_size=32, verbose=0)
    
    print("–†–∞—Ö—É—î–º–æ SHAP values...")
    X_train_flat = X_train.reshape(X_train.shape[0], -1)
    X_test_flat = X_test.reshape(X_test.shape[0], -1)
    background_summary = shap.kmeans(X_train_flat, 20) 

    def predict_wrapper(data_flat):
        data_3d = data_flat.reshape(-1, predict_wrapper, len(dynamic_cols) + len(static_cols))
        return model.predict(data_3d, verbose=0)

    explainer = shap.KernelExplainer(predict_wrapper, background_summary)
    X_test_sample = X_test_flat[:50]
    shap_values = explainer.shap_values(X_test_sample)

    shap_values_3d = shap_values.reshape(-1, 3, len(dynamic_cols) + len(static_cols))
    shap_values_combined = np.sum(shap_values_3d, axis=1)
    
    X_test_sample_3d = X_test_sample.reshape(-1, 3, len(dynamic_cols) + len(static_cols))
    X_test_sample_combined = np.mean(X_test_sample_3d, axis=1)

    # –í–∏–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Ä—è–¥–æ–∫ –∑ feature_names:
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values_combined, X_test_sample_combined, feature_names=dynamic_cols + static_cols)
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    model.save('old_my_heart_model.keras')
    joblib.dump(test_scaler, 'old_user_scaler.pkl')
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–≤–Ω–∏–π —Å–ø–∏—Å–æ–∫ —Ñ—ñ—á
    joblib.dump(dynamic_cols + static_cols, 'old_model_features.pkl') 
    print("‚úÖ –í—Å–µ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")