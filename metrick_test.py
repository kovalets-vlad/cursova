import os

# 1. –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑–º—ñ–Ω–Ω–∏—Ö –æ—Ç–æ—á–µ–Ω–Ω—è –¥–ª—è TensorFlow –∞–±–∏ —É–Ω–∏–∫–∞—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω—å
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['AUTOGRAPH_VERBOSITY'] = '0'

import tensorflow as tf
import logging

# 2. –Ü–≥–Ω–æ—Ä—É–≤–∞–Ω–Ω—è –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω—å –≤—ñ–¥ TensorFlow
tf.get_logger().setLevel(logging.ERROR)
tf.autograph.set_verbosity(0)

import warnings

# 3. –Ü–≥–Ω–æ—Ä—É–≤–∞–Ω–Ω—è –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω—å
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from keras.models import load_model
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error

# ==========================================
# 1. –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø
# ==========================================
DAYS_WINDOW = 3
dynamic_cols = [
    'steps', 'very_active_minutes', 'minutesAsleep', 'sleep_efficiency', 
    'nremhr', 'stress_score', 'nightly_temperature', 'resting_hr',
    'chronic_steps', 'acute_steps', 'acwr' 
]
static_cols = ['age', 'bmi']
weekend_col = ['is_weekend']
target_col = 'hr_delta'
models_info = {}

DIR_PATH = os.path.dirname(os.path.abspath(__file__))
BASE_PATH = os.path.join(DIR_PATH, 'models_ensemble')
DATA_PATH = os.path.join(DIR_PATH, 'daily_fitbit_sema_df_processed.csv')
PLOTS_PATH = os.path.join(BASE_PATH, 'plots')
os.makedirs(PLOTS_PATH, exist_ok=True)

# ==========================================
# 2. –ü–Ü–î–ì–û–¢–û–í–ö–ê "–ß–ò–°–¢–ò–•" –î–ê–ù–ò–• (–ë–ï–ó –°–ö–ï–ô–õ–ò–ù–ì–£)
# ==========================================
def get_clean_user_df(df, user_id):
    """–ì–æ—Ç—É—î DataFrame –∑ —É—Å—ñ–º–∞ —Ñ—ñ—á–∞–º–∏, –∞–ª–µ –ë–ï–ó –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó."""
    user_df = df[df['id'] == user_id].copy()
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–∏—Ö—ñ–¥–Ω–∏—Ö
    if 'date' in user_df.columns:
        user_df['date'] = pd.to_datetime(user_df['date'])
        user_df['is_weekend'] = (user_df['date'].dt.dayofweek >= 5).astype(int)
    else:
        user_df['is_weekend'] = 0

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è —Å—Ç–∞—Ç–∏–∫–∏
    user_df['age'] = pd.to_numeric(user_df['age'], errors='coerce').fillna(30)
    user_df['bmi'] = pd.to_numeric(user_df['bmi'], errors='coerce').fillna(25)
    
    # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
    user_df['chronic_steps'] = user_df['steps'].rolling(window=28, min_periods=1).mean()
    user_df['acute_steps'] = user_df['steps'].rolling(window=7, min_periods=1).mean()
    user_df['acwr'] = user_df['acute_steps'] / (user_df['chronic_steps'] + 1)
    user_df['hr_delta'] = user_df['resting_hr'].diff().fillna(0)
    
    user_df = user_df.ffill().bfill()
    user_df = user_df.iloc[1:].reset_index(drop=True)
    return user_df

# ==========================================
# 3. –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –ê–ù–°–ê–ú–ë–õ–Æ
# ==========================================
def load_models_and_scalers():
    print("üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π —Ç–∞ —ó—Ö —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω–∏—Ö —Å–∫–µ–π–ª–µ—Ä—ñ–≤...")
    for m_name in ['gru', 'lstm', 'cnn']:
        m_path = os.path.join(BASE_PATH, m_name)
        models_info[m_name] = {
            'model': load_model(os.path.join(m_path, f'final_{m_name}_model.keras')),
            'scaler_X': joblib.load(os.path.join(m_path, 'scaler_X.pkl')),
            'scaler_Y': joblib.load(os.path.join(m_path, 'scaler_Y.pkl'))
            # 'model': load_model(f'cursova/models&scaller&features&test/{m_name}3_delta_model.keras'),
            # 'scaler_X': joblib.load(f'cursova/models&scaller&features&test/scaler_X.pkl'),
            # 'scaler_Y': joblib.load(f'cursova/models&scaller&features&test/scaler_Y.pkl')
        }
    print("‚úÖ –£—Å—ñ –º–æ–¥–µ–ª—ñ —Ç–∞ —Å–∫–µ–π–ª–µ—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")

# ==========================================
# 4. –û–¶–Ü–ù–ö–ê
# ==========================================
def evaluate_on_users(user_ids, set_name, df):
    print(f"üìä –û–±—Ä–æ–±–∫–∞ –≤–∏–±—ñ—Ä–∫–∏: {set_name}...")
    all_results = []
    
    for u in user_ids:
        # 1. –û—Ç—Ä–∏–º—É—î–º–æ —á–∏—Å—Ç—ñ –¥–∞–Ω—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
        user_df = get_clean_user_df(df, u)
        if len(user_df) <= DAYS_WINDOW: continue

        model_predictions_bpm = {}
        y_real_bpm = user_df['resting_hr'].values[DAYS_WINDOW:]

        # 2. –ü—Ä–æ–≥–Ω–æ–∑ –∫–æ–∂–Ω–æ—é –º–æ–¥–µ–ª–ª—é –∑—ñ —Å–≤–æ—ó–º —Å–∫–µ–π–ª–µ—Ä–æ–º
        for m_name, tools in models_info.items():
            # –°–∫–µ–π–ª–∏–Ω–≥ –≤—Ö–æ–¥—É —Å–∞–º–µ –¥–ª—è —Ü—ñ—î—ó –º–æ–¥–µ–ª—ñ
            dyn_scaled = tools['scaler_X'].transform(user_df[dynamic_cols].values)
            
            stat_data = user_df[static_cols].values.astype(float)
            stat_data[:, 0] /= 100.0
            stat_data[:, 1] /= 50.0
            week_data = user_df[['is_weekend']].values
            
            X_final_user = np.hstack((dyn_scaled, stat_data, week_data))

            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∫–æ–Ω
            X_wins = []
            for i in range(len(X_final_user) - DAYS_WINDOW):
                X_wins.append(X_final_user[i : i + DAYS_WINDOW])
            X_wins = np.array(X_wins)

            # –ü—Ä–æ–≥–Ω–æ–∑ —Ç–∞ –∑–≤–æ—Ä–æ—Ç–Ω–∏–π —Å–∫–µ–π–ª–∏–Ω–≥
            p_z = tools['model'].predict(X_wins, verbose=0)
            p_delta = tools['scaler_Y'].inverse_transform(p_z).flatten()
            
            # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ BPM (–ü—É–ª—å—Å_–≤—á–æ—Ä–∞ + –î–µ–ª—å—Ç–∞_—Å—å–æ–≥–æ–¥–Ω—ñ)
            prev_bpm = user_df['resting_hr'].values[DAYS_WINDOW-1 : -1]
            model_predictions_bpm[m_name] = prev_bpm + p_delta

        # 3. –ê–Ω—Å–∞–º–±–ª—ñ
        ens_weighted = (model_predictions_bpm['gru'] * 0.25 + 
                        model_predictions_bpm['lstm'] * 0.25 + 
                        model_predictions_bpm['cnn'] * 0.5)
        
        preds_to_eval = {
            'GRU': model_predictions_bpm['gru'],
            'LSTM': model_predictions_bpm['lstm'],
            'CNN': model_predictions_bpm['cnn'],
            'Ens_Weighted': ens_weighted,
        }

        for name, pred in preds_to_eval.items():
            all_results.append({
                "Model": name, 
                "Set": set_name, 
                "MAE": mean_absolute_error(y_real_bpm, pred),
                "RMSE": np.sqrt(mean_squared_error(y_real_bpm, pred)),
                "R2": r2_score(y_real_bpm, pred)
            })

    return pd.DataFrame(all_results)

def save_separate_metrics_plots(report_df):
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å—Ç–∏–ª—é
    sns.set_theme(style="whitegrid")
    
    # --- –ì–†–ê–§–Ü–ö 1: R2 SCORE (–¢–æ—á–Ω—ñ—Å—Ç—å) ---
    plt.figure(figsize=(10, 6))
    ax1 = sns.barplot(x='Model', y='R2', hue='Set', data=report_df, palette='Blues_d')
    
    plt.title('–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∞ –¥–µ—Ç–µ—Ä–º—ñ–Ω–∞—Ü—ñ—ó ($R^2$)', fontsize=14, pad=15)
    plt.ylabel('–ó–Ω–∞—á–µ–Ω–Ω—è R^2 (0.0 - 1.0)')
    plt.xlabel('–ú–æ–¥–µ–ª—å')
    plt.ylim(0.8, 1.0) # –ú–∞—Å—à—Ç–∞–± –¥–ª—è –Ω–∞–æ—á–Ω–æ—Å—Ç—ñ —Ä—ñ–∑–Ω–∏—Ü—ñ
    
    # –î–æ–¥–∞—î–º–æ —Ü–∏—Ñ—Ä–∏ –Ω–∞–¥ —Å—Ç–æ–≤–ø—á–∏–∫–∞–º–∏
    for container in ax1.containers:
        ax1.bar_label(container, fmt='%.2f', padding=3)
    
    plt.tight_layout()
    fig_path = os.path.join(PLOTS_PATH, 'models_accuracy_r2.png')
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.close() # –ó–∞–∫—Ä–∏–≤–∞—î–º–æ, —â–æ–± –Ω–µ –≤–∏–≤–æ–¥–∏—Ç–∏ –≤ –∫–æ–Ω—Å–æ–ª—å, –∞ —Ç—ñ–ª—å–∫–∏ –∑–±–µ—Ä–µ–≥—Ç–∏
    print(f"‚úÖ –ì—Ä–∞—Ñ—ñ–∫ R2 –∑–±–µ—Ä–µ–∂–µ–Ω–æ —è–∫ '{fig_path}'")

    # --- –ì–†–ê–§–Ü–ö 2: MAE (–ü–æ–º–∏–ª–∫–∞) ---
    plt.figure(figsize=(10, 6))
    ax2 = sns.barplot(x='Model', y='MAE', hue='Set', data=report_df, palette='Reds_d')
    
    plt.title('–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å–µ—Ä–µ–¥–Ω—å–æ—ó –∞–±—Å–æ–ª—é—Ç–Ω–æ—ó –ø–æ–º–∏–ª–∫–∏ (MAE)', fontsize=14, pad=15)
    plt.ylabel('–ü–æ–º–∏–ª–∫–∞ (BPM)')
    plt.xlabel('–ú–æ–¥–µ–ª—å')
    
    # –î–æ–¥–∞—î–º–æ —Ü–∏—Ñ—Ä–∏ –Ω–∞–¥ —Å—Ç–æ–≤–ø—á–∏–∫–∞–º–∏
    for container in ax2.containers:
        ax2.bar_label(container, fmt='%.2f', padding=3)
    
    plt.tight_layout()
    fig_path = os.path.join(PLOTS_PATH, 'models_error_mae.png')
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ –ì—Ä–∞—Ñ—ñ–∫ MAE –∑–±–µ—Ä–µ–∂–µ–Ω–æ —è–∫ '{fig_path}'")

def analyze_error_smoothing(user_ids, df, set_name="Test"):
    """
    –ê–Ω–∞–ª—ñ–∑—É—î —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ–º–∏–ª–æ–∫ —Ç–∞ –µ—Ñ–µ–∫—Ç –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è –∞–Ω—Å–∞–º–±–ª–µ–º.
    """
    print(f"üîç –î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –ø–æ–º–∏–ª–æ–∫ –¥–ª—è –≤–∏–±—ñ—Ä–∫–∏: {set_name}...")
    all_data = []

    for u in user_ids:
        user_df = get_clean_user_df(df, u)
        if len(user_df) <= DAYS_WINDOW: continue

        y_real = user_df['resting_hr'].values[DAYS_WINDOW:]
        prev_bpm = user_df['resting_hr'].values[DAYS_WINDOW-1 : -1]
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø—Ä–æ–≥–Ω–æ–∑–∏ –∫–æ–∂–Ω–æ—ó –º–æ–¥–µ–ª—ñ
        preds = {}
        for m_name, tools in models_info.items():
            dyn_scaled = tools['scaler_X'].transform(user_df[dynamic_cols].values)
            stat_data = user_df[static_cols].values.astype(float)
            stat_data[:, 0] /= 100.0; stat_data[:, 1] /= 50.0
            X_final = np.hstack((dyn_scaled, stat_data, user_df[['is_weekend']].values))
            X_wins = np.array([X_final[i : i + DAYS_WINDOW] for i in range(len(X_final) - DAYS_WINDOW)])
            
            p_z = tools['model'].predict(X_wins, verbose=0)
            p_delta = tools['scaler_Y'].inverse_transform(p_z).flatten()
            preds[m_name] = prev_bpm + p_delta

        # –ê–Ω—Å–∞–º–±–ª—å (—Å–µ—Ä–µ–¥–Ω—î)
        ens_pred = (preds['gru'] + preds['lstm'] + preds['cnn']) / 3

        # –§–æ—Ä–º—É—î–º–æ DataFrame –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        for i in range(len(y_real)):
            row_base = {"Actual_BPM": y_real[i], "User": u}
            for m in ['gru', 'lstm', 'cnn']:
                err = abs(y_real[i] - preds[m][i])
                all_data.append({**row_base, "Model": m.upper(), "Error": err, "Type": "Single"})
            
            ens_err = abs(y_real[i] - ens_pred[i])
            all_data.append({**row_base, "Model": "ENSEMBLE", "Error": ens_err, "Type": "Ensemble"})

    analysis_df = pd.DataFrame(all_data)

    # --- –ì–†–ê–§–Ü–ö 1: –†–û–ó–ü–û–î–Ü–õ –ü–û–ú–ò–õ–û–ö (VIOLIN PLOT) ---
    plt.figure(figsize=(12, 6))
    sns.violinplot(x='Model', y='Error', data=analysis_df, inner="quartile", palette="muted")
    plt.title(f'–†–æ–∑–ø–æ–¥—ñ–ª –ø–æ–º–∏–ª–æ–∫: –ß–∏ –∑–≥–ª–∞–¥–∂—É—î –ê–Ω—Å–∞–º–±–ª—å? ({set_name})')
    plt.ylabel('–ê–±—Å–æ–ª—é—Ç–Ω–∞ –ø–æ–º–∏–ª–∫–∞ (BPM)')
    plt.ylim(0, analysis_df['Error'].quantile(0.99)) # –í—ñ–¥—Å—ñ–∫–∞—î–º–æ –≤–∏–∫–∏–¥–∏ –¥–ª—è –Ω–∞–æ—á–Ω–æ—Å—Ç—ñ
    plt.savefig(os.path.join(PLOTS_PATH, 'error_distribution_violin.png'))
    plt.show()

    # --- –ì–†–ê–§–Ü–ö 2: –î–ï –°–ê–ú–ï –ú–û–î–ï–õ–Ü –ü–û–ú–ò–õ–Ø–Æ–¢–¨–°–Ø (Error vs Actual) ---
    plt.figure(figsize=(12, 6))
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ä–µ–≥—Ä–µ—Å—ñ–π–Ω—É –ª—ñ–Ω—ñ—é, —â–æ–± –ø–æ–±–∞—á–∏—Ç–∏ —Ç—Ä–µ–Ω–¥ –ø–æ–º–∏–ª–∫–∏
    for m in ['LSTM', 'ENSEMBLE']:
        subset = analysis_df[analysis_df['Model'] == m]
        sns.regplot(x='Actual_BPM', y='Error', data=subset, scatter=False, label=f'–¢—Ä–µ–Ω–¥ –ø–æ–º–∏–ª–∫–∏ {m}')
    
    plt.title('–ß–∏ –∑—Ä–æ—Å—Ç–∞—î –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–º—ñ–Ω—ñ –ø—É–ª—å—Å—É?')
    plt.xlabel('–†–µ–∞–ª—å–Ω–∏–π –ø—É–ª—å—Å (BPM)')
    plt.ylabel('–°–µ—Ä–µ–¥–Ω—è –ø–æ–º–∏–ª–∫–∞')
    plt.legend()
    plt.savefig(os.path.join(PLOTS_PATH, 'error_trend_by_bpm.png'))
    plt.show()

    # –í–∏–≤—ñ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è
    std_errors = analysis_df.groupby('Model')['Error'].std()
    print("\nüìâ –°—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª–µ–π (Standard Deviation of Error):")
    print(std_errors.to_string())

def analyze_model_diversity(user_ids, df):
    print("üß† –ê–Ω–∞–ª—ñ–∑ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó –ø–æ–º–∏–ª–æ–∫ (Diversity Analysis)...")
    residuals_data = {m: [] for m in ['gru', 'lstm', 'cnn']}
    
    for u in user_ids:
        user_df = get_clean_user_df(df, u) #
        if len(user_df) <= DAYS_WINDOW: continue

        y_real = user_df['resting_hr'].values[DAYS_WINDOW:]
        prev_bpm = user_df['resting_hr'].values[DAYS_WINDOW-1 : -1]

        for m_name, tools in models_info.items():
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –ø—Ä–æ—Ü–µ—Å –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø—Ä–æ–≥–Ω–æ–∑—É (—è–∫ —É –≤–∞—à–æ–º—É –∫–æ–¥—ñ)
            dyn_scaled = tools['scaler_X'].transform(user_df[dynamic_cols].values)
            stat_data = user_df[static_cols].values.astype(float)
            stat_data[:, 0] /= 100.0; stat_data[:, 1] /= 50.0
            X_final = np.hstack((dyn_scaled, stat_data, user_df[['is_weekend']].values))
            X_wins = np.array([X_final[i : i + DAYS_WINDOW] for i in range(len(X_final) - DAYS_WINDOW)])
            
            p_z = tools['model'].predict(X_wins, verbose=0)
            p_delta = tools['scaler_Y'].inverse_transform(p_z).flatten()
            pred_bpm = prev_bpm + p_delta
            
            # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –∑–∞–ª–∏—à–∫—ñ–≤ (residuals)
            res = y_real - pred_bpm
            residuals_data[m_name].extend(res)

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è DataFrame —ñ–∑ –∑–∞–ª–∏—à–∫–∞–º–∏
    res_df = pd.DataFrame(residuals_data)
    corr_matrix = res_df.corr()

    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    plt.figure(figsize=(8, 6))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", vmin=0, vmax=1)
    plt.title('–ö–æ—Ä–µ–ª—è—Ü—ñ—è –ø–æ–º–∏–ª–æ–∫ –º—ñ–∂ –º–æ–¥–µ–ª—è–º–∏\n')
    plt.show()

    return corr_matrix

if __name__ == "__main__":
    load_models_and_scalers()
    df_full = pd.read_csv(DATA_PATH)
    all_users = df_full['id'].unique()
    train_users = all_users[:int(len(all_users)*0.8)]
    test_users = all_users[int(len(all_users)*0.8):]

    train_results = evaluate_on_users(train_users, "Train", df_full)
    test_results = evaluate_on_users(test_users, "Test", df_full)

    final_report = pd.concat([train_results, test_results]).groupby(['Model', 'Set']).mean().reset_index()
    final_report['MAE'] = final_report['MAE'].round(2)
    final_report['R2'] = final_report['R2'].round(2)
    final_report['RMSE'] = final_report['RMSE'].round(2)
    print("\n" + "="*60)
    print("–ó–í–Ü–¢ –¢–û–ß–ù–û–°–¢–Ü –ú–û–î–ï–õ–ï–ô")
    print("="*60)
    print(final_report.sort_values(by=['Model', 'Set']).to_string(index=False))

    save_separate_metrics_plots(final_report)
    analyze_error_smoothing(test_users, df_full, "Test")
    corr_res = analyze_model_diversity(test_users, df_full)
    print("\n–ö–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è –ø–æ–º–∏–ª–æ–∫ –º–æ–¥–µ–ª–µ–π:")
    print(corr_res.to_string())